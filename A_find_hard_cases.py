import os
import json
import torch
import numpy as np
from PIL import Image
from torchvision.transforms import functional as F
from tqdm import tqdm
import sys
sys.path.append("src") # ç¡®ä¿èƒ½æ‰¾åˆ° dataset å’Œ model


from model import ST_SAM, Baseline_SAM2

# ======= é…ç½® =======
FOLD = 4  # æˆ‘ä»¬å°±å» Fold 4 (Infrared3) é‡Œæ‰¾
IMG_SIZE = 1024
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

CKPT_BASE = f"./checkpoints_ablation/fold_{FOLD}/best_model.pth"
CKPT_OURS = f"./checkpoints_gal50_bk/fold_{FOLD}/best_model.pth"                       
YOLO_JSON = f"./data_splits/yolo_boxes_fold{FOLD}.json"
SPLIT_JSON = f"./data_splits/fold_{FOLD}.json"

def compute_dice(pred_np, mask_np):
    inter = np.sum(pred_np * mask_np)
    return (2. * inter) / (np.sum(pred_np) + np.sum(mask_np) + 1e-6)

def load_model(model, ckpt_path):
    state_dict = torch.load(ckpt_path, map_location=DEVICE)
    model.load_state_dict({k.replace("module.", ""): v for k, v in state_dict.items()})
    model.eval()
    return model

print("â³ åŠ è½½æ¨¡å‹ä¸­...")
model_base = load_model(Baseline_SAM2(checkpoint_path="./checkpoints/sam2_hiera_large.pt").to(DEVICE), CKPT_BASE)
model_ours = load_model(ST_SAM(checkpoint_path="./checkpoints/sam2_hiera_large.pt").to(DEVICE), CKPT_OURS)

with open(YOLO_JSON, 'r') as f: yolo_preds = json.load(f)
with open(SPLIT_JSON, 'r') as f: split_data = json.load(f)

results = []

print("ğŸ” æ­£åœ¨å…¨è‡ªåŠ¨æ‰«æéªŒè¯é›†ï¼Œå¯»æ‰¾æœ€ä½³å¯¹æ¯”å›¾...")
for item in tqdm(split_data['val']):
    img_id = item['id']
    if img_id not in yolo_preds: continue
        
    img_path = item['image']
    label_path = item['label'].replace("/Label/", "/Cleaned_Label/")
    
    # è¯»å–æ•°æ®
    image = Image.open(img_path).convert("RGB").resize((IMG_SIZE, IMG_SIZE), resample=Image.BILINEAR)
    label = Image.open(label_path).convert("L").resize((IMG_SIZE, IMG_SIZE), resample=Image.NEAREST)
    img_tensor = F.to_tensor(image).unsqueeze(0).to(DEVICE)
    lbl_np = (np.array(label) > 127).astype(np.uint8)
    
    # YOLO æ¡†
    box_norm = yolo_preds[img_id]
    box = [box_norm[0]*IMG_SIZE, box_norm[1]*IMG_SIZE, box_norm[2]*IMG_SIZE, box_norm[3]*IMG_SIZE]
    box_tensor = torch.tensor([box], dtype=torch.float32).to(DEVICE)

    # æ¨ç†
    with torch.no_grad():
        pred_base = (torch.sigmoid(model_base(img_tensor, box_tensor)) > 0.5).cpu().numpy()[0, 0]
        pred_ours = (torch.sigmoid(model_ours(img_tensor, box_tensor)) > 0.5).cpu().numpy()[0, 0]
        
    dice_base = compute_dice(pred_base, lbl_np)
    dice_ours = compute_dice(pred_ours, lbl_np)
    
    # è®¡ç®—å·®è· (æˆ‘ä»¬å¸Œæœ›æ‰¾ Baseline ç¿»è½¦ï¼Œä½† Ours åšæŒºçš„å›¾)
    gap = dice_ours - dice_base
    results.append({'id': img_id, 'path': img_path, 'lbl_path': label_path, 'base': dice_base, 'ours': dice_ours, 'gap': gap})

# æŒ‰ Gap é™åºæ’åº
results.sort(key=lambda x: x['gap'], reverse=True)

print("\nğŸ† å¼ºçƒˆå»ºè®®ä½¿ç”¨ä»¥ä¸‹å›¾åƒæ”¾å…¥ visualize.py è¿›è¡Œå¯è§†åŒ–ï¼š")
for i, res in enumerate(results[:10]):
    print(f"Top {i+1}: {res['id']} | Baseline Dice: {res['base']:.4f} | ST-SAM Dice: {res['ours']:.4f} | å·®è·: +{res['gap']:.4f}")