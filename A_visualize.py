import os
import json
import torch
import numpy as np
import cv2
import matplotlib.pyplot as plt
from PIL import Image
from torchvision.transforms import functional as F
from matplotlib.patches import Rectangle
import sys
sys.path.append("src") # ç¡®ä¿èƒ½æ‰¾åˆ° dataset å’Œ model

# å¯¼å…¥ä½ çš„æ¨¡å‹ (è¯·ç¡®ä¿ model.py é‡Œæœ‰è¿™ä¸‰ä¸ªç±»)
from model import ST_SAM, Baseline_SAM2
try:
    from model import MSA_Baseline_SAM2
except ImportError:
    print("âš ï¸ æœªæ‰¾åˆ° MSA_Baseline_SAM2ï¼Œè¯·ç¡®ä¿å®ƒåœ¨ model.py ä¸­å®šä¹‰ã€‚")

# =========================================================
# é…ç½®åŒºåŸŸ (è¯·æ ¹æ®ä½ éœ€è¦å¯è§†åŒ–çš„å›¾ç‰‡è¿›è¡Œä¿®æ”¹)
# =========================================================
FOLD = 4  # ä½ æƒ³ä½¿ç”¨å“ªä¸ª Fold çš„æƒé‡
IMG_SIZE = 1024
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# æƒé‡è·¯å¾„
CKPT_BASE = f"./checkpoints_ablation/fold_{FOLD}/best_model.pth" # æ›¿æ¢ä¸ºä½ çš„çœŸå®è·¯å¾„
CKPT_MSA  = f"./checkpoints_msa/fold_{FOLD}/best_model.pth"                    # æ›¿æ¢ä¸ºä½ çš„çœŸå®è·¯å¾„
CKPT_OURS = f"./checkpoints_gal50_bk/fold_{FOLD}/best_model.pth"                        # æ›¿æ¢ä¸ºä½ çš„çœŸå®è·¯å¾„

# YOLO é¢„æµ‹æ¡† JSON è·¯å¾„
YOLO_JSON = f"./data_splits/yolo_boxes_fold{FOLD}.json"

# =========================================================
# è¾…åŠ©å‡½æ•°
# =========================================================
def load_model_weights(model, ckpt_path):
    if not os.path.exists(ckpt_path):
        print(f"âŒ æ‰¾ä¸åˆ°æƒé‡: {ckpt_path}")
        return model
    state_dict = torch.load(ckpt_path, map_location=DEVICE)
    new_state_dict = {k.replace("module.", ""): v for k, v in state_dict.items()}
    model.load_state_dict(new_state_dict)
    model.eval()
    return model

def overlay_mask(image_np, mask_np, color, alpha=0.6):
    """å°†äºŒå€¼ Mask å åŠ åˆ° RGB å›¾åƒä¸Š"""
    overlay = image_np.copy()
    for c in range(3):
        overlay[:, :, c] = np.where(mask_np > 0, image_np[:, :, c] * (1 - alpha) + color[c] * alpha, image_np[:, :, c])
    return overlay

def get_zoom_bbox(mask_np, padding=50):
    """æ ¹æ® GT Mask è‡ªåŠ¨è·å–å±€éƒ¨æ”¾å¤§çš„ BBox"""
    y_indices, x_indices = np.where(mask_np > 0)
    if len(y_indices) == 0:
        return 0, 0, mask_np.shape[1], mask_np.shape[0]
    
    x_min, x_max = np.min(x_indices), np.max(x_indices)
    y_min, y_max = np.min(y_indices), np.max(y_indices)
    
    x_min = max(0, x_min - padding)
    y_min = max(0, y_min - padding)
    x_max = min(mask_np.shape[1], x_max + padding)
    y_max = min(mask_np.shape[0], y_max + padding)
    
    return x_min, y_min, x_max, y_max

# =========================================================
# ä¸»æ¨æ–­ä¸ç»˜å›¾å‡½æ•°
# =========================================================
def visualize_image(img_path, label_path, img_id):
    print(f"ğŸ” æ­£åœ¨å¤„ç†å›¾åƒ: {img_id}")
    
    # 1. åŠ è½½å›¾åƒå’Œæ ‡ç­¾
    image = Image.open(img_path).convert("RGB")
    label = Image.open(label_path).convert("L")
    
    image = image.resize((IMG_SIZE, IMG_SIZE), resample=Image.BILINEAR)
    label = label.resize((IMG_SIZE, IMG_SIZE), resample=Image.NEAREST)
    
    img_tensor = F.to_tensor(image).unsqueeze(0).to(DEVICE)
    lbl_np = (np.array(label) > 127).astype(np.uint8)
    
    # 2. åŠ è½½ YOLO Box
    with open(YOLO_JSON, 'r') as f:
        yolo_preds = json.load(f)
    if img_id in yolo_preds:
        box_norm = yolo_preds[img_id]
        box = [box_norm[0] * IMG_SIZE, box_norm[1] * IMG_SIZE, box_norm[2] * IMG_SIZE, box_norm[3] * IMG_SIZE]
    else:
        print("âš ï¸ æœªæ‰¾åˆ° YOLO æ¡†ï¼Œä½¿ç”¨å…¨å±€æ¡†ã€‚")
        box = [0, 0, IMG_SIZE, IMG_SIZE]
    box_tensor = torch.tensor([box], dtype=torch.float32).to(DEVICE)

    # 3. åŠ è½½æ¨¡å‹å¹¶æ¨æ–­
    print("â³ æ­£åœ¨è¿è¡Œ SAM Baseline...")
    model_base = load_model_weights(Baseline_SAM2(checkpoint_path="./checkpoints/sam2_hiera_large.pt").to(DEVICE), CKPT_BASE)
    
    print("â³ æ­£åœ¨è¿è¡Œ SAM MSA...")
    try:
        model_msa = load_model_weights(MSA_Baseline_SAM2(checkpoint_path="./checkpoints/sam2_hiera_large.pt").to(DEVICE), CKPT_MSA)
    except:
        model_msa = None

    print("â³ æ­£åœ¨è¿è¡Œ ST-SAM (Ours)...")
    model_ours = load_model_weights(ST_SAM(checkpoint_path="./checkpoints/sam2_hiera_large.pt").to(DEVICE), CKPT_OURS)

    with torch.no_grad():
        pred_base = (torch.sigmoid(model_base(img_tensor, box_tensor)) > 0.5).cpu().numpy()[0, 0]
        pred_msa = (torch.sigmoid(model_msa(img_tensor, box_tensor)) > 0.5).cpu().numpy()[0, 0] if model_msa else np.zeros_like(pred_base)
        pred_ours = (torch.sigmoid(model_ours(img_tensor, box_tensor)) > 0.5).cpu().numpy()[0, 0]

    # 4. å›¾åƒæ¸²æŸ“
    img_np = np.array(image)
    
    # å®šä¹‰é¢œè‰² (RGB): GT(ç»¿è‰²), Base(è“è‰²), MSA(æ©™è‰²), Ours(çº¢è‰²)
    c_gt, c_base, c_msa, c_ours = (0, 255, 0), (0, 100, 255), (255, 165, 0), (255, 0, 0)
    
    vis_gt = overlay_mask(img_np, lbl_np, c_gt)
    vis_base = overlay_mask(img_np, pred_base, c_base)
    vis_msa = overlay_mask(img_np, pred_msa, c_msa)
    vis_ours = overlay_mask(img_np, pred_ours, c_ours)

    # è·å–å±€éƒ¨æ”¾å¤§åŒºåŸŸ (Zoom-in Box)
    x1, y1, x2, y2 = get_zoom_bbox(lbl_np, padding=80)

    # 5. ä½¿ç”¨ Matplotlib æ‹¼å›¾ (æ’ç‰ˆä¸º: åŸå›¾+GT | Base | MSA | Ours)
    fig, axes = plt.subplots(2, 4, figsize=(20, 10), gridspec_kw={'height_ratios': [2, 1]})
    plt.subplots_adjust(wspace=0.05, hspace=0.05)
    
    titles = ["Ground Truth", "SAM Baseline", "SAM MSA", "ST-SAM (Ours)"]
    images_to_show = [vis_gt, vis_base, vis_msa, vis_ours]

    for i in range(4):
        # ç¬¬ä¸€æ’ï¼šå…¨å›¾
        ax_full = axes[0, i]
        ax_full.imshow(images_to_show[i])
        ax_full.set_title(titles[i], fontsize=16, fontweight='bold', pad=10)
        ax_full.axis('off')
        # åœ¨å…¨å›¾ä¸Šç”»å‡º Zoom-in çš„æ¡†
        rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='yellow', facecolor='none', linestyle='--')
        ax_full.add_patch(rect)

        # ç¬¬äºŒæ’ï¼šå±€éƒ¨æ”¾å¤§å›¾ (Zoom-in)
        ax_zoom = axes[1, i]
        ax_zoom.imshow(images_to_show[i][y1:y2, x1:x2])
        ax_zoom.axis('off')
        # ç»™æ”¾å¤§å›¾åŠ ä¸ªè¾¹æ¡†
        for spine in ax_zoom.spines.values():
            spine.set_edgecolor('yellow')
            spine.set_linewidth(3)
            spine.set_visible(True)

    save_name = f"qualitative_result_{img_id}.png"
    plt.savefig(save_name, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"âœ… å¯è§†åŒ–ç»“æœå·²ä¿å­˜è‡³: {save_name}\n")
    plt.close()

if __name__ == "__main__":
    # =========================================================
    # åœ¨è¿™é‡Œå¡«å…¥ä½ æƒ³å¯è§†åŒ–çš„çº¢å¤–å›¾åƒè·¯å¾„ï¼
    # å¼ºçƒˆå»ºè®®æŒ‘é€‰ 3-4 å¼ çº¢å¤–ä¸­å¿ƒ (Infrared) ä¸­ï¼Œé•¿æ¡å½¢çŠ¶æ˜æ˜¾ä¸”æœ‰åå…‰å¹²æ‰°çš„å›¾
    # =========================================================
    test_cases = [
        {
            "img_path": "../Unet/dataset/Infrared3/Original/Infrared3_000588.PNG",     # æ›¿æ¢ä¸ºä½ çš„çœŸå®å›¾åƒè·¯å¾„
            "label_path": "../Unet/dataset/Infrared3/Cleaned_Label/Infrared3_000588.PNG", # æ›¿æ¢ä¸ºä½ çš„çœŸå®æ ‡ç­¾è·¯å¾„
            "img_id": "Infrared3_000588"                                # å¯¹åº” YOLO JSON é‡Œçš„ ID
        },
        {
            "img_path": "../Unet/dataset/Infrared3/Original/Infrared3_000146.PNG",     # æ›¿æ¢ä¸ºä½ çš„çœŸå®å›¾åƒè·¯å¾„
            "label_path": "../Unet/dataset/Infrared3/Cleaned_Label/Infrared3_000146.PNG", # æ›¿æ¢ä¸ºä½ çš„çœŸå®æ ‡ç­¾è·¯å¾„
            "img_id": "Infrared3_000146"                                # å¯¹åº” YOLO JSON é‡Œçš„ ID
        },
        {
            "img_path": "../Unet/dataset/Infrared3/Original/Infrared3_000189.PNG",     # æ›¿æ¢ä¸ºä½ çš„çœŸå®å›¾åƒè·¯å¾„
            "label_path": "../Unet/dataset/Infrared3/Cleaned_Label/Infrared3_000189.PNG", # æ›¿æ¢ä¸ºä½ çš„çœŸå®æ ‡ç­¾è·¯å¾„
            "img_id": "Infrared3_000189"                                # å¯¹åº” YOLO JSON é‡Œçš„ ID
        },
    ]
    
    for case in test_cases:
        if os.path.exists(case["img_path"]) and os.path.exists(case["label_path"]):
            visualize_image(case["img_path"], case["label_path"], case["img_id"])
        else:
            print(f"âš ï¸ æ‰¾ä¸åˆ°å›¾åƒæˆ–æ ‡ç­¾æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„: {case['img_path']}")