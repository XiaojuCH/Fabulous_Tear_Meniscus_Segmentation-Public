import os
import argparse
import json
import time
import datetime
import csv
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, destroy_process_group, all_reduce, ReduceOp
from torch.amp import GradScaler, autocast
import torch.nn.functional as F
from tqdm import tqdm

import warnings
warnings.filterwarnings("ignore")

from dataset import TearDataset
# å¼•ç”¨ä½ çš„æœ€ç»ˆç‰ˆæ¨¡å‹
from model import Baseline_SAM2

# ==============================================================================
# é…ç½®åŒºåŸŸ
# ==============================================================================
CONFIG = {
    "batch_size": 8,
    "num_workers": 4,
    "lr": 1e-4,
    "epochs": 50, # è·¨æ¨¡æ€é€šå¸¸æ”¶æ•›å¿«ï¼Œ50å¤Ÿäº†ï¼Œæƒ³è·‘æ»¡100ä¹Ÿè¡Œ
    "img_size": 1024,
    "model_name": "SAM (Cross-Modality Test)",
    "optimizer": "AdamW",
    "loss": "Dice + BCE",
    "gpu_count": 8 # æ ¹æ®ä½ å®é™…æƒ…å†µè°ƒæ•´
}

def setup_ddp():
    if "RANK" in os.environ:
        init_process_group(backend="nccl")
        rank = int(os.environ["RANK"])
        local_rank = int(os.environ["LOCAL_RANK"])
        world_size = int(os.environ["WORLD_SIZE"])
        num_gpus = torch.cuda.device_count()
        torch.cuda.set_device(local_rank % num_gpus)
        return rank, local_rank % num_gpus, world_size
    else:
        return 0, 0, 1

def cleanup():
    if "RANK" in os.environ:
        destroy_process_group()

class DiceBCELoss(nn.Module):
    def __init__(self, weight=None, size_average=True):
        super(DiceBCELoss, self).__init__()

    def forward(self, inputs, targets, smooth=1):
        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='mean')
        inputs_sigmoid = torch.sigmoid(inputs)
        inputs_flat = inputs_sigmoid.view(-1)
        targets_flat = targets.view(-1)
        intersection = (inputs_flat * targets_flat).sum()
        dice_loss = 1 - (2.*intersection + smooth)/(inputs_flat.sum() + targets_flat.sum() + smooth)
        return 0.5 * bce_loss + 0.5 * dice_loss

# ==============================================================================
# æ ¸å¿ƒï¼šè‡ªåŠ¨æ„å»ºè·¨æ¨¡æ€æ•°æ®é›†
# ==============================================================================
def get_cross_modal_data(mode="train_color_test_ir"):
    """
    éå†æ‰€æœ‰ fold çš„ jsonï¼Œåˆå¹¶åæ ¹æ®æ–‡ä»¶åé‡Œçš„å…³é”®å­—å¼ºè¡Œæ‹†åˆ†ã€‚
    """
    all_data = []
    # 1. æŠŠæ‰€æœ‰æ•°æ®æ”¶é›†èµ·æ¥ (åˆ©ç”¨ fold_0 åˆ° fold_4 çš„ val é›†äº’æ–¥ä¸”äº’è¡¥çš„ç‰¹æ€§)
    for i in range(5):
        json_path = f"./data_splits/fold_{i}.json"
        if os.path.exists(json_path):
            with open(json_path, 'r') as f:
                split = json.load(f)
                # LOCO åè®®ä¸­ï¼ŒéªŒè¯é›†æ˜¯ä¸é‡å¤çš„ï¼ŒæŠŠæ‰€æœ‰ fold çš„ val åŠ èµ·æ¥å°±æ˜¯å…¨é›†
                all_data.extend(split['val'])
        else:
            print(f"âš ï¸ Warning: {json_path} not found.")

    print(f"ğŸ“¦ Total images found: {len(all_data)}")
    
    # 2. æ ¹æ®æ–‡ä»¶åè¿‡æ»¤
    # å‡è®¾æ–‡ä»¶åç±»ä¼¼: "Color1_xxx.jpg" æˆ– "Infrared1_xxx.jpg"
    color_data = []
    ir_data = []
    
    for item in all_data:
        # æ£€æŸ¥ image è·¯å¾„å­—ç¬¦ä¸²
        img_path = item['image'] if isinstance(item, dict) else item
        
        # ä½ çš„æ–‡ä»¶åç‰¹å¾ï¼šColor vs Infrared
        if "Color" in img_path:
            color_data.append(item)
        elif "Infrared" in img_path:
            ir_data.append(item)
            
    print(f"ğŸ¨ Color Images: {len(color_data)}")
    print(f"ğŸŒ‘ Infrared Images: {len(ir_data)}")
    
    # 3. æ ¹æ®æ¨¡å¼è¿”å›
    if mode == "train_color_test_ir":
        print("ğŸ‘‰ Setting: Train on [Color] -> Test on [Infrared]")
        return color_data, ir_data
    elif mode == "train_ir_test_color":
        print("ğŸ‘‰ Setting: Train on [Infrared] -> Test on [Color]")
        return ir_data, color_data
    else:
        raise ValueError("Unknown mode")

# ==============================================================================
# è®­ç»ƒä¸»å¾ªç¯
# ==============================================================================
def main(mode):
    rank, local_rank, world_size = setup_ddp()
    is_master = (rank == 0)

    # è·å–è·¨æ¨¡æ€æ•°æ®
    train_list, val_list = get_cross_modal_data(mode)
    
    train_dataset = TearDataset(train_list, mode='train', img_size=CONFIG['img_size'])
    val_dataset = TearDataset(val_list, mode='val', img_size=CONFIG['img_size'])

    train_sampler = DistributedSampler(train_dataset, shuffle=True)
    val_sampler = DistributedSampler(val_dataset, shuffle=False)

    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], sampler=train_sampler, num_workers=CONFIG['num_workers'], pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], sampler=val_sampler, num_workers=CONFIG['num_workers'], pin_memory=True)

    model = Baseline_SAM2(checkpoint_path="./checkpoints/sam2_hiera_large.pt").to(local_rank)
    model = nn.SyncBatchNorm.convert_sync_batchnorm(model)
    model = DDP(model, device_ids=[local_rank], find_unused_parameters=True)

    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=CONFIG['lr'])
    criterion = DiceBCELoss().to(local_rank)
    scaler = GradScaler('cuda') 

    best_dice = 0.0
    
    # ä¿å­˜ç›®å½•åŒºåˆ†æ¨¡å¼
    save_dir = f"./checkpoints_cross_modal/{mode}"
    if is_master:
        os.makedirs(save_dir, exist_ok=True)
        print(f"ğŸš€ Start Training: {mode}")

    for epoch in range(CONFIG['epochs']):
        model.train()
        train_sampler.set_epoch(epoch)
        
        train_loss = 0.0
        pbar = tqdm(train_loader, disable=not is_master, desc=f"Epoch {epoch+1}/{CONFIG['epochs']}")

        for batch in pbar:
            images = batch['image'].to(local_rank, non_blocking=True)
            labels = batch['label'].to(local_rank, non_blocking=True)
            boxes = batch['box'].to(local_rank, non_blocking=True)

            optimizer.zero_grad()
            with autocast('cuda'):
                preds = model(images, boxes)
                loss = criterion(preds, labels)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            train_loss += loss.item()
            pbar.set_postfix(loss=loss.item())

        # Validation (Testing on the OTHER modality)
        model.eval()
        val_dice = 0.0
        with torch.no_grad():
            for batch in val_loader:
                images = batch['image'].to(local_rank, non_blocking=True)
                labels = batch['label'].to(local_rank, non_blocking=True)
                boxes = batch['box'].to(local_rank, non_blocking=True)

                with autocast('cuda'):
                    preds = model(images, boxes)
                    preds = torch.sigmoid(preds)
                    preds_bin = (preds > 0.5).float()
                
                intersection = (preds_bin * labels).sum()
                dice = (2. * intersection) / (preds_bin.sum() + labels.sum() + 1e-6)
                val_dice += dice.item()

        # Reduce metrics
        val_dice_tensor = torch.tensor(val_dice).to(local_rank)
        all_reduce(val_dice_tensor, op=ReduceOp.SUM)
        avg_val_dice = val_dice_tensor.item() / (len(val_loader) * world_size)

        if is_master:
            print(f"Epoch {epoch+1} | Val Dice ({mode.split('_')[-1].upper()}): {avg_val_dice:.4f}")
            
            if avg_val_dice > best_dice:
                best_dice = avg_val_dice
                torch.save(model.module.state_dict(), f"{save_dir}/best_model.pth")
                print(f"ğŸ”¥ New Best Dice: {best_dice:.4f} -> Saved!")

    cleanup()

# ä¿®æ”¹ train_cross_modal.py çš„åº•éƒ¨ä»£ç 

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # æ¨¡å¼é€‰æ‹©
    parser.add_argument("--mode", type=str, default="train_color_test_ir", help="Experiment mode")
    
    # ã€æ–°å¢ã€‘æ¥æ”¶ DDP è‡ªåŠ¨ä¼ å…¥çš„ local-rank å‚æ•°
    # è™½ç„¶æˆ‘ä»¬åœ¨ setup_ddp é‡Œç”¨çš„æ˜¯ os.environï¼Œä½†å¿…é¡»è¿™é‡Œå ä¸ªä½ï¼Œé˜²æ­¢æŠ¥é”™
    parser.add_argument("--local_rank", type=int, default=0, help="Local rank for DDP") 
    # æ³¨æ„ï¼šæœ‰æ—¶å€™æ˜¯ --local-rank (ä¸­é—´æ˜¯æ¨ªæ )ï¼Œargparse ä¼šè‡ªåŠ¨è½¬ä¸ºä¸‹åˆ’çº¿ local_rank
    # ä¸ºäº†ä¿é™©ï¼Œå¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å†™æ³•å…¼å®¹ï¼š
    parser.add_argument("--local-rank", type=int, default=0, dest="local_rank")

    args = parser.parse_args()
    
    main(mode=args.mode)