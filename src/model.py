import torch
import torch.nn as nn
import torch.nn.functional as F
from sam2.build_sam import build_sam2
import os

# å¼•å…¥ä½ åˆšæ‰å†™å¥½çš„æ–°æ³¨æ„åŠ›æ¨¡å—
# from New_att import StripDetailAdapter # æ—§ç‰ˆ
# from New_att_v2 import GatedStripAdapter
# from New_att_v3 import GatedDilatedStripAdapter
# from NNew_att_v2_plus import GatedMultiScaleStripAdapter
# from NNew_att_v2_plus_plus import GatedMultiScaleStripAdapter
# from NNew_att_v2_PPP import GatedMultiScaleStripAdapter
# from NNew_att_v2_4P import GatedMultiScaleStripAdapter
# from NNNew_att_v2_PPPGPT import GSCSA
# from NNNew_att_v2_PPPGPT_final_bk import GSCSA
# from NNNew_att_v2_PPPGPT_final_bk import GSCSA

from NNNew_att_GAL_bk import GAL_Adapter

# from NNNew_att_GAL_Notin import GAL_Adapter
# from NNNew_att_GAL_V2 import GAL_Adapter
# from NNNew_att_GAL_V3 import GAL_Adapter
# from NNNew_att_GAL_V4 import GAL_Adapter
# from NNNew_att_GAL_V5 import GAL_Adapter
# from NNNew_att_GAL_V6 import GAL_Adapter

# ==============================================================================
# ä¸»æ¨¡å‹ï¼šST-SAM (High-Res Injection Version)
# ==============================================================================
class ST_SAM(nn.Module):
    def __init__(self, 
                 model_cfg="sam2_hiera_l.yaml", 
                 checkpoint_path="./checkpoints/sam2_hiera_large.pt"
                 ):
        super().__init__()
        
        # 1. åŠ è½½ SAM2 éª¨å¹²
        if not os.path.exists(checkpoint_path):
            if os.path.exists(f"../{checkpoint_path}"):
                checkpoint_path = f"../{checkpoint_path}"
            else:
                 print(f"âš ï¸ Warning: Checkpoint not found at {checkpoint_path}")

        self.sam2 = build_sam2(model_cfg, checkpoint_path)
        
        # 2. å†»ç»“å¤§éƒ¨åˆ†å‚æ•° (PEFT ç­–ç•¥)
        for param in self.sam2.image_encoder.parameters():
            param.requires_grad = False
        for param in self.sam2.sam_prompt_encoder.parameters():
            param.requires_grad = False
        for param in self.sam2.memory_attention.parameters():
            param.requires_grad = False

        # ---------------------------------------------------------
        # 3. åˆå§‹åŒ–è‡ªå®šä¹‰æ¨¡å—
        # ---------------------------------------------------------
        # æŠ•å½±å±‚ï¼šå°† Backbone çš„ 256 ç»´ç‰¹å¾é™ç»´ï¼Œå¯¹é½ Decoder éœ€æ±‚
        self.proj_s0 = nn.Conv2d(256, 32, kernel_size=1, bias=False)
        self.proj_s1 = nn.Conv2d(256, 64, kernel_size=1, bias=False)
        
# æ¢å¤çœŸÂ·V1 çš„å¤§æ„Ÿå—é‡ï¼Œä»¥ä¿è¯å®è§‚ä¸è¿æ–­è£‚
        # å¼•å…¥ reduction=8 ä½œä¸ºå¼ºæ­£åˆ™åŒ–å™¨ï¼Œå¼ºåˆ¶æŠ¹å¹³é—¨æ§ç½‘ç»œå¯¹åå…‰çš„æ•æ„Ÿåº¦æŠ–åŠ¨ï¼
        
        # ã€s0 å±‚ (Stride 4)ã€‘: 
        self.adapter_s0 = GAL_Adapter(
            in_channels=32, 
            kernel_size_large=23,  # æ¢å¤æœ€ä¼˜å¤§æ„Ÿå—é‡
            kernel_size_small=7, 
            reduction=8            # ğŸš€ ç»ˆææ€æ‰‹é”ï¼šé™ä½é€šé“è‡ªç”±åº¦ï¼Œå¼ºè¿«è¾¹ç¼˜å¹³æ»‘
        )
        
        # ã€s1 å±‚ (Stride 8)ã€‘: 
        self.adapter_s1 = GAL_Adapter(
            in_channels=64, 
            kernel_size_large=23,  # æ¢å¤æœ€ä¼˜å¤§æ„Ÿå—é‡
            kernel_size_small=7,
            reduction=8            # ğŸš€ ç»ˆææ€æ‰‹é”
        )

        # ---------------------------------------------------------
        # 4. å¼€å¯éœ€è¦è®­ç»ƒéƒ¨åˆ†çš„æ¢¯åº¦
        # ---------------------------------------------------------
        # 4.1 Mask Decoder
        for param in self.sam2.sam_mask_decoder.parameters():
            param.requires_grad = True
            
        # 4.2 æŠ•å½±å±‚ & æ–°åŠ å…¥çš„ Adapter
        trainable_layers = [self.proj_s0, self.proj_s1, self.adapter_s0, self.adapter_s1]
        for layer in trainable_layers:
            for param in layer.parameters():
                param.requires_grad = True

    def forward(self, images, box_prompts):
        """
        images: [B, 3, 1024, 1024]
        box_prompts: [B, 4]
        """
        # 1. Image Encoder (Frozen)
        with torch.no_grad():
            backbone_out = self.sam2.image_encoder(images)
            src_features = backbone_out["vision_features"] # Bottleneck feature (1/32)
            
            # è·å–å¤šå°ºåº¦ç‰¹å¾
            _fpn_features = backbone_out["backbone_fpn"]
            raw_s0 = _fpn_features[0] # Stride 4 (256x256)
            raw_s1 = _fpn_features[1] # Stride 8 (128x128)

        # 2. High-Res Injection (Trainable)
        # -------------------------------------------------------
        # Step A: æŠ•å½± (Channel Projection)
        feat_s0 = self.proj_s0(raw_s0)  # [B, 32, 256, 256]
        feat_s1 = self.proj_s1(raw_s1)  # [B, 64, 128, 128]
        
        # Step B: æ³¨å…¥æ¡çŠ¶æ³¨æ„åŠ› (Strip Attention Injection)
        # è¿™é‡Œæ˜¯å…³é”®ï¼åœ¨é€å…¥ Decoder ä¹‹å‰ï¼Œå…ˆå¢å¼ºè¾¹ç•Œç‰¹å¾
        refined_s0 = self.adapter_s0(feat_s0) 
        refined_s1 = self.adapter_s1(feat_s1)
        
        high_res_features = [refined_s0, refined_s1]
        # -------------------------------------------------------

        # 3. Prompt Encoder (Frozen)
        sparse_embeddings, dense_embeddings = self.sam2.sam_prompt_encoder(
            points=None,
            boxes=box_prompts,
            masks=None,
        )

        # 4. Mask Decoder (Trainable)
        low_res_masks, iou_predictions, _, _ = self.sam2.sam_mask_decoder(
            image_embeddings=src_features, # ç“¶é¢ˆå±‚ç‰¹å¾ä¿æŒåŸæ ·
            image_pe=self.sam2.sam_prompt_encoder.get_dense_pe(),
            sparse_prompt_embeddings=sparse_embeddings,
            dense_prompt_embeddings=dense_embeddings,
            multimask_output=False,
            repeat_image=False,
            high_res_features=high_res_features # ä¼ å…¥å¢å¼ºåçš„é«˜åˆ†è¾¨ç‡ç‰¹å¾
        )
        
        # 5. Upscale
        masks = F.interpolate(
            low_res_masks, 
            size=(images.shape[2], images.shape[3]), 
            mode="bilinear", 
            align_corners=False
        )
        
        return masks
# ==============================================================================
# å¯¹æ¯”æ¨¡å‹ï¼šBaseline SAM 2 (æ—  Adapterï¼Œä»…å¾®è°ƒ Decoder)
# ==============================================================================
class Baseline_SAM2(nn.Module):
    def __init__(self, 
                 model_cfg="sam2_hiera_l.yaml", 
                 checkpoint_path="./checkpoints/sam2_hiera_large.pt"
                 ):
        super().__init__()
        
        # 1. åŠ è½½ SAM2
        if not os.path.exists(checkpoint_path):
            if os.path.exists(f"../{checkpoint_path}"):
                checkpoint_path = f"../{checkpoint_path}"
            else:
                 print(f"âš ï¸ Warning: Checkpoint not found at {checkpoint_path}")

        self.sam2 = build_sam2(model_cfg, checkpoint_path)
        
        # 2. å†»ç»“å¤§éƒ¨åˆ†å‚æ•° (ä¿æŒå’Œ ST-SAM ä¸€è‡´)
        for param in self.sam2.image_encoder.parameters():
            param.requires_grad = False
        for param in self.sam2.sam_prompt_encoder.parameters():
            param.requires_grad = False
        for param in self.sam2.memory_attention.parameters():
            param.requires_grad = False

        # ---------------------------------------------------------
        # 3. ç§»é™¤ Adapterï¼Œä»…ä¿ç•™å¿…è¦çš„ç»´åº¦æŠ•å½±
        # ---------------------------------------------------------
        # self.adapter = StripAttentionAdapter(...)  <-- ã€åˆ é™¤ã€‘åˆ›æ–°æ¨¡å—
        
        # ä¿ç•™è¿™ä¸¤ä¸ªæŠ•å½±å±‚æ˜¯ä¸ºäº†è®© Backbone çš„ç‰¹å¾ç»´åº¦èƒ½å¯¹é½ Decoder çš„è¾“å…¥è¦æ±‚
        # è¿™å±äºç»“æ„é€‚é…ï¼Œä¸å±äºâ€œåˆ›æ–°ç‚¹â€ï¼Œæ‰€ä»¥ Baseline é‡Œè¦ç•™ç€
        self.proj_s0 = nn.Conv2d(256, 32, kernel_size=1, bias=False)
        self.proj_s1 = nn.Conv2d(256, 64, kernel_size=1, bias=False)
        
        # ---------------------------------------------------------
        # 4. å¼€å¯æ¢¯åº¦
        # ---------------------------------------------------------
        # 4.1 Mask Decoder (SAM2 åŸç”Ÿéƒ¨åˆ†)
        for param in self.sam2.sam_mask_decoder.parameters():
            param.requires_grad = True
            
        # 4.2 æŠ•å½±å±‚
        for param in self.proj_s0.parameters():
            param.requires_grad = True
        for param in self.proj_s1.parameters():
            param.requires_grad = True

    def forward(self, images, box_prompts):
        # 1. Image Encoder (Frozen)
        with torch.no_grad():
            backbone_out = self.sam2.image_encoder(images)
            src_features = backbone_out["vision_features"]
            _fpn_features = backbone_out["backbone_fpn"]
            raw_s0 = _fpn_features[0]
            raw_s1 = _fpn_features[1]

        # æŠ•å½±ç»´åº¦ (ä¿æŒç»“æ„ä¸€è‡´)
        feat_s0 = self.proj_s0(raw_s0) 
        feat_s1 = self.proj_s1(raw_s1)
        high_res_features = [feat_s0, feat_s1]

        # ---------------------------------------------------------
        # 2. ã€å…³é”®ä¿®æ”¹ã€‘ç›´æ¥ä½¿ç”¨åŸå§‹ç‰¹å¾ï¼Œä¸ç»è¿‡ Adapter
        # ---------------------------------------------------------
        # refined_features = self.adapter(src_features) <-- ã€åˆ é™¤ã€‘
        refined_features = src_features  # <-- ã€æ–°å¢ã€‘ç›´æ¥é€ä¼ 
        
        # 3. Prompt Encoder (Frozen)
        sparse_embeddings, dense_embeddings = self.sam2.sam_prompt_encoder(
            points=None,
            boxes=box_prompts,
            masks=None,
        )

        # 4. Mask Decoder (Trainable)
        low_res_masks, iou_predictions, _, _ = self.sam2.sam_mask_decoder(
            image_embeddings=refined_features, # è¿™é‡Œè¾“å…¥çš„æ˜¯æ²¡æœ‰åŠ å¼ºè¿‡çš„ç‰¹å¾
            image_pe=self.sam2.sam_prompt_encoder.get_dense_pe(),
            sparse_prompt_embeddings=sparse_embeddings,
            dense_prompt_embeddings=dense_embeddings,
            multimask_output=False,
            repeat_image=False,
            high_res_features=high_res_features 
        )
        
        # 5. Upscale
        masks = F.interpolate(
            low_res_masks, 
            size=(images.shape[2], images.shape[3]), 
            mode="bilinear", 
            align_corners=False
        )
        
        return masks

import math

# ==============================================================================
# è¾…åŠ©æ¨¡å—ï¼šæ ‡å‡† LoRA çº¿æ€§å±‚
# ==============================================================================
class LoRALinear(nn.Module):
    def __init__(self, original_linear, rank=4, alpha=4):
        super().__init__()
        # ä¿æŒåŸæœ‰å…¨è¿æ¥å±‚ï¼Œå¹¶å½»åº•å†»ç»“
        self.linear = original_linear
        for param in self.linear.parameters():
            param.requires_grad = False
            
        self.in_features = original_linear.in_features
        self.out_features = original_linear.out_features
        self.rank = rank
        self.scaling = alpha / rank

        # LoRA çš„ A å’Œ B çŸ©é˜µ
        self.lora_A = nn.Parameter(torch.zeros(self.in_features, rank))
        self.lora_B = nn.Parameter(torch.zeros(rank, self.out_features))
        
        # åˆå§‹åŒ–ï¼šA ç”¨ Kaimingï¼ŒB å…¨é›¶ï¼Œä¿è¯åˆå§‹çŠ¶æ€ç­‰æ•ˆäºåŸç½‘ç»œ
        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
        nn.init.zeros_(self.lora_B)

    def forward(self, x):
        # åŸæœ‰è¾“å‡º + ä½ç§©çŸ©é˜µè¾“å‡º
        orig_out = self.linear(x)
        lora_out = (x @ self.lora_A @ self.lora_B) * self.scaling
        return orig_out + lora_out

def inject_lora_to_decoder(module, rank=4):
    """é€’å½’åœ°å°† Mask Decoder ä¸­çš„ nn.Linear æ›¿æ¢ä¸º LoRALinear"""
    for name, child in module.named_children():
        if isinstance(child, nn.Linear):
            # æ›¿æ¢çº¿æ€§å±‚
            setattr(module, name, LoRALinear(child, rank=rank))
        else:
            inject_lora_to_decoder(child, rank=rank)

# ==============================================================================
# å¯¹æ¯”æ¨¡å‹ 1ï¼šSAM 2 + LoRA Baseline
# ==============================================================================
class LoRA_SAM2(nn.Module):
    def __init__(self, 
                 model_cfg="sam2_hiera_l.yaml", 
                 checkpoint_path="./checkpoints/sam2_hiera_large.pt",
                 lora_rank=4
                 ):
        super().__init__()
        
        # 1. åŠ è½½ SAM2 éª¨å¹²
        if not os.path.exists(checkpoint_path) and os.path.exists(f"../{checkpoint_path}"):
            checkpoint_path = f"../{checkpoint_path}"
            
        self.sam2 = build_sam2(model_cfg, checkpoint_path)
        
        # 2. å…¨é¢å†»ç»“åŸç”Ÿå‚æ•°
        for param in self.sam2.parameters():
            param.requires_grad = False

        # 3. ç»´åº¦æŠ•å½±å±‚ (ä¿ç•™ï¼Œä¸ºäº†èƒ½å¤Ÿå¯¹é½è¾“å…¥ï¼Œè¿™æ˜¯å¿…è¦çš„æ¶æ„å¦¥å)
        self.proj_s0 = nn.Conv2d(256, 32, kernel_size=1, bias=False)
        self.proj_s1 = nn.Conv2d(256, 64, kernel_size=1, bias=False)
        
        # 4. æ³¨å…¥ LoRA åˆ° Mask Decoder
        inject_lora_to_decoder(self.sam2.sam_mask_decoder, rank=lora_rank)
        
        # 5. å¼€å¯å¿…è¦æ¢¯åº¦çš„è¿½è¸ª
        # ä»…å¼€å¯æˆ‘ä»¬è‡ªå·±åŠ çš„æŠ•å½±å±‚å’Œ LoRA å‚æ•°çš„æ¢¯åº¦
        for param in self.proj_s0.parameters():
            param.requires_grad = True
        for param in self.proj_s1.parameters():
            param.requires_grad = True
            
        # LoRA å‚æ•°é»˜è®¤ requires_grad=Trueï¼Œæ‰€ä»¥ç›´æ¥ç­›é€‰å³å¯
        
    def forward(self, images, box_prompts):
        with torch.no_grad():
            backbone_out = self.sam2.image_encoder(images)
            src_features = backbone_out["vision_features"]
            _fpn_features = backbone_out["backbone_fpn"]
            raw_s0 = _fpn_features[0] 
            raw_s1 = _fpn_features[1] 

        # æŠ•å½±
        feat_s0 = self.proj_s0(raw_s0)  
        feat_s1 = self.proj_s1(raw_s1)  
        high_res_features = [feat_s0, feat_s1]

        with torch.no_grad():
            sparse_embeddings, dense_embeddings = self.sam2.sam_prompt_encoder(
                points=None, boxes=box_prompts, masks=None,
            )

        # å¸¦æœ‰ LoRA çš„ Mask Decoder æ¨ç†
        low_res_masks, iou_predictions, _, _ = self.sam2.sam_mask_decoder(
            image_embeddings=src_features, 
            image_pe=self.sam2.sam_prompt_encoder.get_dense_pe(),
            sparse_prompt_embeddings=sparse_embeddings,
            dense_prompt_embeddings=dense_embeddings,
            multimask_output=False,
            repeat_image=False,
            high_res_features=high_res_features 
        )
        
        masks = F.interpolate(low_res_masks, size=(images.shape[2], images.shape[3]), mode="bilinear", align_corners=False)
        return masks


# ==============================================================================
# è¾…åŠ©æ¨¡å—ï¼šé€šç”¨çš„ç“¶é¢ˆ Adapter (MSA é£æ ¼)
# ==============================================================================
class MSA_Adapter(nn.Module):
    def __init__(self, in_channels, reduction=4):
        super().__init__()
        mid_channels = max(in_channels // reduction, 16)
        # ä¸€ä¸ªéå¸¸æ ‡å‡†çš„å·ç§¯ç“¶é¢ˆç»“æ„ï¼Œæ²¡æœ‰ç‰¹æ®Šçš„èŠ±æ ·
        self.bottleneck = nn.Sequential(
            nn.Conv2d(in_channels, mid_channels, kernel_size=1, bias=False),
            nn.BatchNorm2d(mid_channels),
            nn.GELU(),
            nn.Conv2d(mid_channels, mid_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(mid_channels),
            nn.GELU(),
            nn.Conv2d(mid_channels, in_channels, kernel_size=1, bias=False)
        )

    def forward(self, x):
        return x + self.bottleneck(x)

# ==============================================================================
# å¯¹æ¯”æ¨¡å‹ 2ï¼šSAM 2 + ç»å…¸ç“¶é¢ˆ Adapter (MSA Baseline)
# ==============================================================================
class MSA_Baseline_SAM2(nn.Module):
    def __init__(self, 
                 model_cfg="sam2_hiera_l.yaml", 
                 checkpoint_path="./checkpoints/sam2_hiera_large.pt"
                 ):
        super().__init__()
        
        # 1. åŠ è½½å†»ç»“çš„ SAM2 ä¸»å¹²
        if not os.path.exists(checkpoint_path) and os.path.exists(f"../{checkpoint_path}"):
            checkpoint_path = f"../{checkpoint_path}"
        self.sam2 = build_sam2(model_cfg, checkpoint_path)
        
        for param in self.sam2.image_encoder.parameters():
            param.requires_grad = False
        for param in self.sam2.sam_prompt_encoder.parameters():
            param.requires_grad = False
        for param in self.sam2.memory_attention.parameters():
            param.requires_grad = False

        # 2. æŠ•å½±å±‚
        self.proj_s0 = nn.Conv2d(256, 32, kernel_size=1, bias=False)
        self.proj_s1 = nn.Conv2d(256, 64, kernel_size=1, bias=False)
        
        # 3. æ³¨å…¥ç»å…¸ Adapter (è€Œä¸æ˜¯ä½ çš„ GAL)
        self.adapter_s0 = MSA_Adapter(in_channels=32)
        self.adapter_s1 = MSA_Adapter(in_channels=64)

        # 4. å¼€å¯æ¢¯åº¦ (æ³¨æ„ï¼šè¿™é‡Œå’Œä½ çš„ SAM_Gal ä¸€æ ·ï¼Œå¼€å¯äº†æ•´ä¸ª Decoder çš„å¾®è°ƒï¼)
        for param in self.sam2.sam_mask_decoder.parameters():
            param.requires_grad = True
            
        trainable_layers = [self.proj_s0, self.proj_s1, self.adapter_s0, self.adapter_s1]
        for layer in trainable_layers:
            for param in layer.parameters():
                param.requires_grad = True

    def forward(self, images, box_prompts):
        with torch.no_grad():
            backbone_out = self.sam2.image_encoder(images)
            src_features = backbone_out["vision_features"]
            _fpn_features = backbone_out["backbone_fpn"]
            raw_s0 = _fpn_features[0]
            raw_s1 = _fpn_features[1]

        # ç‰¹å¾æ³¨å…¥æµç¨‹
        feat_s0 = self.proj_s0(raw_s0)
        feat_s1 = self.proj_s1(raw_s1)
        
        # ç»è¿‡ç»å…¸çš„ MSA Adapter
        refined_s0 = self.adapter_s0(feat_s0)
        refined_s1 = self.adapter_s1(feat_s1)
        high_res_features = [refined_s0, refined_s1]

        with torch.no_grad():
            sparse_embeddings, dense_embeddings = self.sam2.sam_prompt_encoder(
                points=None, boxes=box_prompts, masks=None,
            )

        # Decoder æ¨ç†
        low_res_masks, iou_predictions, _, _ = self.sam2.sam_mask_decoder(
            image_embeddings=src_features,
            image_pe=self.sam2.sam_prompt_encoder.get_dense_pe(),
            sparse_prompt_embeddings=sparse_embeddings,
            dense_prompt_embeddings=dense_embeddings,
            multimask_output=False,
            repeat_image=False,
            high_res_features=high_res_features
        )

        masks = F.interpolate(low_res_masks, size=(images.shape[2], images.shape[3]), mode="bilinear", align_corners=False)
        return masks


# ==============================================================================
# å¯¹æ¯”æ¨¡å‹ï¼šMedSAM-style (SAM2-Hiera-L + å…¨é‡ Mask Decoder å¾®è°ƒ)
# å‚è€ƒ: Ma et al., "Segment Anything in Medical Images", Nature Communications 2024
# å¯¹é½ç­–ç•¥: å†»ç»“ Image Encoderï¼Œä»…å¾®è°ƒ Mask Decoder + æŠ•å½±å±‚ï¼Œæ— ä»»ä½•è‡ªå®šä¹‰ Adapter
# ==============================================================================
class MedSAM_SAM2(nn.Module):
    def __init__(self,
                 model_cfg="sam2_hiera_l.yaml",
                 checkpoint_path="./checkpoints/sam2_hiera_large.pt"):
        super().__init__()

        if not os.path.exists(checkpoint_path):
            if os.path.exists(f"../{checkpoint_path}"):
                checkpoint_path = f"../{checkpoint_path}"
            else:
                print(f"âš ï¸ Warning: Checkpoint not found at {checkpoint_path}")

        self.sam2 = build_sam2(model_cfg, checkpoint_path)

        # å†»ç»“ Image Encoderï¼ˆä¸ MedSAM å®˜æ–¹ç­–ç•¥ä¸€è‡´ï¼‰
        for param in self.sam2.image_encoder.parameters():
            param.requires_grad = False
        for param in self.sam2.sam_prompt_encoder.parameters():
            param.requires_grad = False
        for param in self.sam2.memory_attention.parameters():
            param.requires_grad = False

        # ç»´åº¦æŠ•å½±å±‚ï¼ˆç»“æ„é€‚é…ï¼Œä¸ Baseline_SAM2 å®Œå…¨ä¸€è‡´ï¼‰
        self.proj_s0 = nn.Conv2d(256, 32, kernel_size=1, bias=False)
        self.proj_s1 = nn.Conv2d(256, 64, kernel_size=1, bias=False)

        # å¼€å¯ Mask Decoder å…¨é‡å¾®è°ƒï¼ˆMedSAM æ ¸å¿ƒç­–ç•¥ï¼‰
        for param in self.sam2.sam_mask_decoder.parameters():
            param.requires_grad = True
        for param in self.proj_s0.parameters():
            param.requires_grad = True
        for param in self.proj_s1.parameters():
            param.requires_grad = True

    def forward(self, images, box_prompts):
        # 1. Image Encoder (Frozen)
        with torch.no_grad():
            backbone_out = self.sam2.image_encoder(images)
            src_features = backbone_out["vision_features"]
            _fpn_features = backbone_out["backbone_fpn"]
            raw_s0 = _fpn_features[0]
            raw_s1 = _fpn_features[1]

        # 2. ç»´åº¦æŠ•å½±ï¼ˆæ—  Adapterï¼Œç›´æ¥é€ä¼ ï¼‰
        feat_s0 = self.proj_s0(raw_s0)
        feat_s1 = self.proj_s1(raw_s1)
        high_res_features = [feat_s0, feat_s1]

        # 3. Prompt Encoder (Frozen)
        sparse_embeddings, dense_embeddings = self.sam2.sam_prompt_encoder(
            points=None,
            boxes=box_prompts,
            masks=None,
        )

        # 4. Mask Decoder (Trainable - MedSAM æ ¸å¿ƒ)
        low_res_masks, iou_predictions, _, _ = self.sam2.sam_mask_decoder(
            image_embeddings=src_features,
            image_pe=self.sam2.sam_prompt_encoder.get_dense_pe(),
            sparse_prompt_embeddings=sparse_embeddings,
            dense_prompt_embeddings=dense_embeddings,
            multimask_output=False,
            repeat_image=False,
            high_res_features=high_res_features,
        )

        # 5. Upscale to original resolution
        masks = F.interpolate(
            low_res_masks,
            size=(images.shape[2], images.shape[3]),
            mode="bilinear",
            align_corners=False,
        )
        return masks