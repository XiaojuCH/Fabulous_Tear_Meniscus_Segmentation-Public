import torch
import torch.nn as nn
import torch.nn.functional as F
from sam2.build_sam import build_sam2
import os

# ==============================================================================
# åˆ›æ–°æ¨¡å—ï¼šStrip-Topology Attention Adapter (ST-Adapter)
# ==============================================================================
class StripAttentionAdapter(nn.Module):
    def __init__(self, in_channels, reduction=16):
        super().__init__()
        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))
        self.pool_w = nn.AdaptiveAvgPool2d((1, None))

        mid_channels = max(16, in_channels // reduction)

        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(mid_channels)
        self.act1 = nn.ReLU(inplace=True)
        
        self.conv_h = nn.Conv2d(mid_channels, in_channels, kernel_size=1, bias=False)
        self.conv_w = nn.Conv2d(mid_channels, in_channels, kernel_size=1, bias=False)
        self.sigmoid = nn.Sigmoid()
        
        self.final_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1, bias=False)
        nn.init.zeros_(self.final_conv.weight)

    def forward(self, x):
        identity = x
        x_h = self.pool_h(x)
        x_w = self.pool_w(x)
        
        y_h = self.act1(self.bn1(self.conv1(x_h)))
        y_w = self.act1(self.bn1(self.conv1(x_w)))
        
        w_h = self.sigmoid(self.conv_h(y_h))
        w_w = self.sigmoid(self.conv_w(y_w))
        
        attended_features = identity * w_h * w_w
        out = identity + self.final_conv(attended_features)
        return out

# ==============================================================================
# ä¸»æ¨¡å‹ï¼šST-SAM
# ==============================================================================
class ST_SAM(nn.Module):
    def __init__(self, 
                 model_cfg="sam2_hiera_l.yaml", 
                 checkpoint_path="./checkpoints/sam2_hiera_large.pt"
                 ):
        super().__init__()
        
        # ---------------------------------------------------------
        # 1. å¿…é¡»æœ€å…ˆåŠ è½½ SAM2 æ¨¡å‹ (è¿™ä¸€æ­¥ç»å¯¹ä¸èƒ½å°‘ï¼)
        # ---------------------------------------------------------
        if not os.path.exists(checkpoint_path):
            if os.path.exists(f"../{checkpoint_path}"):
                checkpoint_path = f"../{checkpoint_path}"
            else:
                 print(f"âš ï¸ Warning: Checkpoint not found at {checkpoint_path}")

        # ã€å…³é”®ä¿®å¤ã€‘è¿™è¡Œä»£ç å¿…é¡»è¢«æ‰§è¡Œï¼Œä¸èƒ½æ˜¯æ³¨é‡Š
        self.sam2 = build_sam2(model_cfg, checkpoint_path)
        
        # ---------------------------------------------------------
        # 2. å†»ç»“å¤§éƒ¨åˆ†å‚æ•°
        # ---------------------------------------------------------
        for param in self.sam2.image_encoder.parameters():
            param.requires_grad = False
        for param in self.sam2.sam_prompt_encoder.parameters():
            param.requires_grad = False
        for param in self.sam2.memory_attention.parameters():
            param.requires_grad = False

        # ---------------------------------------------------------
        # 3. åˆå§‹åŒ–è‡ªå®šä¹‰æ¨¡å— (Adapter å’Œ Projection)
        # ---------------------------------------------------------
        self.feature_dim = 256 
        self.adapter = StripAttentionAdapter(self.feature_dim)
        
        # ã€ä¹‹å‰ä¿®å¤çš„é€šé“æŠ•å½±å±‚ã€‘
        # feat_s0: 256 -> 32
        self.proj_s0 = nn.Conv2d(256, 32, kernel_size=1, bias=False)
        # feat_s1: 256 -> 64
        self.proj_s1 = nn.Conv2d(256, 64, kernel_size=1, bias=False)
        
        # ---------------------------------------------------------
        # 4. å¼€å¯éœ€è¦è®­ç»ƒéƒ¨åˆ†çš„æ¢¯åº¦
        # ---------------------------------------------------------
        # 4.1 Adapter
        for param in self.adapter.parameters():
            param.requires_grad = True
            
        # 4.2 Mask Decoder (SAM2 åŸç”Ÿéƒ¨åˆ†)
        for param in self.sam2.sam_mask_decoder.parameters():
            param.requires_grad = True
            
        # 4.3 æ–°å¢çš„æŠ•å½±å±‚
        for param in self.proj_s0.parameters():
            param.requires_grad = True
        for param in self.proj_s1.parameters():
            param.requires_grad = True

    def forward(self, images, box_prompts):
        """
        images: [B, 3, 1024, 1024]
        box_prompts: [B, 4]
        """
        # 1. Image Encoder (Frozen)
        with torch.no_grad():
            backbone_out = self.sam2.image_encoder(images)
            src_features = backbone_out["vision_features"]
            
            # è·å–åŸå§‹ FPN ç‰¹å¾ [256, 256, 256]
            _fpn_features = backbone_out["backbone_fpn"]
            
            # å–å‰ä¸¤å±‚ (Stride 4 å’Œ Stride 8)
            # æ³¨æ„ï¼š_fpn_features[0] æ˜¯ Stride 4, _fpn_features[1] æ˜¯ Stride 8
            raw_s0 = _fpn_features[0]
            raw_s1 = _fpn_features[1]

        # ã€æ–°å¢ä¿®å¤ 2ã€‘è¿›è¡Œç»´åº¦æŠ•å½± (256 -> 32/64)
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å¼€å¯æ¢¯åº¦ï¼Œæ‰€ä»¥è¦åœ¨ no_grad ä¹‹å¤–
        feat_s0 = self.proj_s0(raw_s0)  # [B, 32, 256, 256]
        feat_s1 = self.proj_s1(raw_s1)  # [B, 64, 128, 128]
        
        # ç»„åˆæˆåˆ—è¡¨ä¼ å…¥
        high_res_features = [feat_s0, feat_s1]

        # 2. Strip Adapter (Trainable)
        refined_features = self.adapter(src_features)
        
        # 3. Prompt Encoder (Frozen)
        sparse_embeddings, dense_embeddings = self.sam2.sam_prompt_encoder(
            points=None,
            boxes=box_prompts,
            masks=None,
        )

        # 4. Mask Decoder (Trainable)
        low_res_masks, iou_predictions, _, _ = self.sam2.sam_mask_decoder(
            image_embeddings=refined_features, 
            image_pe=self.sam2.sam_prompt_encoder.get_dense_pe(),
            sparse_prompt_embeddings=sparse_embeddings,
            dense_prompt_embeddings=dense_embeddings,
            multimask_output=False,
            repeat_image=False,
            high_res_features=high_res_features 
        )
        
        # 5. Upscale
        masks = F.interpolate(
            low_res_masks, 
            size=(images.shape[2], images.shape[3]), 
            mode="bilinear", 
            align_corners=False
        )
        
        return masks
# ==============================================================================
# æµ‹è¯•ä»£ç å— (ç”¨äºæ£€æŸ¥æ¨¡å‹ç»“æ„å’Œå‰å‘ä¼ æ’­æ˜¯å¦é€šç•…)
# ==============================================================================
if __name__ == "__main__":
    # éœ€è¦å…ˆä¸‹è½½æƒé‡æ‰èƒ½è¿è¡Œæ­¤æµ‹è¯•
    # å‡è®¾æƒé‡å·²åœ¨ correct path
    try:
        # 1. å®ä¾‹åŒ–æ¨¡å‹
        # æ³¨æ„ï¼šéœ€è¦ç¡®ä¿å½“å‰ç›®å½•ä¸‹æœ‰ sam2_hiera_l.yaml é…ç½®æ–‡ä»¶
        # é€šå¸¸å®‰è£… sam2 åº“åä¼šè‡ªåŠ¨æ‰¾åˆ°ï¼Œæ‰¾ä¸åˆ°éœ€æ‰‹åŠ¨æŒ‡å®šç»å¯¹è·¯å¾„
        model = ST_SAM(checkpoint_path="../checkpoints/sam2_hiera_large.pt").cuda()
        
        # 2. åˆ›å»º dummyè¾“å…¥
        batch_size = 2
        dummy_img = torch.randn(batch_size, 3, 1024, 1024).cuda()
        dummy_box = torch.tensor([[100, 100, 500, 500]] * batch_size).float().cuda()
        
        # 3. å‰å‘ä¼ æ’­æµ‹è¯•
        print("\nğŸ§ª å¼€å§‹å‰å‘ä¼ æ’­æµ‹è¯•...")
        output_masks = model(dummy_img, dummy_box)
        
        print(f"âœ… è¾“å‡º Shape: {output_masks.shape}") # æœŸæœ›: [B, 1, 1024, 1024]
        
        # 4. æ£€æŸ¥æ¢¯åº¦çŠ¶å†µ
        print("\nğŸ” æ£€æŸ¥æ¢¯åº¦è¦æ±‚:")
        for name, param in model.named_parameters():
            if param.requires_grad:
                # åªæ‰“å°å¯è®­ç»ƒçš„å±‚ï¼Œçœ‹çœ‹ adapter æ˜¯å¦åœ¨é‡Œé¢
                if "adapter" in name or "mask_decoder" in name:
                    print(f"  -> Trainable: {name}")

    except FileNotFoundError as e:
        print(f"\nâš ï¸ æµ‹è¯•è·³è¿‡: {e}")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")