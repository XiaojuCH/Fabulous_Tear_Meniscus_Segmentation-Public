import torch
import torch.nn as nn
import torch.nn.functional as F

class DWConv(nn.Module):
    def __init__(self, dim, kernel_size, padding, dilation=1):
        super().__init__()
        self.dw = nn.Conv2d(
            dim, dim, 
            kernel_size=kernel_size, 
            padding=padding, 
            dilation=dilation, 
            groups=dim, 
            bias=False
        )
        self.bn = nn.BatchNorm2d(dim)
        self.act = nn.GELU()

    def forward(self, x):
        return self.act(self.bn(self.dw(x)))

class GAL_Adapter(nn.Module):
    def __init__(self, in_channels, kernel_size_large=23, kernel_size_small=7, reduction=4):
        super().__init__()
        self.proj_in = nn.Conv2d(in_channels, in_channels, 1, bias=False)

        self.pre_orient = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(in_channels),
            nn.GELU()
        )

        pad_l = (kernel_size_large - 1) // 2
        pad_s = (kernel_size_small - 1) // 2
        self.strip_h_large = DWConv(in_channels, (kernel_size_large, 1), (pad_l, 0))
        self.strip_w_large = DWConv(in_channels, (1, kernel_size_large), (0, pad_l))
        self.strip_h_small = DWConv(in_channels, (kernel_size_small, 1), (pad_s, 0))
        self.strip_w_small = DWConv(in_channels, (1, kernel_size_small), (0, pad_s))

        self.local_3x3 = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(in_channels),
            nn.GELU()
        )
        self.local_5x5 = DWConv(in_channels, 3, padding=2, dilation=2)

        # ğŸš€ V2ï¼šç©ºé—´é—¨æ§ï¼Œè¾“å‡º [B, 6, H, W] è€Œé [B, 6C, H, W]
        # ç”¨ 7x7 å¤§æ ¸å·ç§¯æå–èåˆåçš„ç©ºé—´è½®å»“ï¼Œè®©æ¨¡å‹æ„ŸçŸ¥ç»†é•¿æ‹“æ‰‘
        self.spatial_gating = nn.Sequential(
            nn.Conv2d(in_channels * 6, in_channels, kernel_size=1, bias=False),
            nn.BatchNorm2d(in_channels),
            nn.GELU(),
            nn.Conv2d(in_channels, 6, kernel_size=7, padding=3, bias=False)
        )

        self.style_fc = nn.Sequential(
            nn.Conv2d(in_channels * 2, in_channels, 1), 
            nn.GELU(),
            nn.Conv2d(in_channels, in_channels * 2, 1)  
        )

        self.proj_out = nn.Conv2d(in_channels, in_channels, 1, bias=False)

    def forward(self, x):
        shortcut = x
        x = self.proj_in(x)

        x_oriented = self.pre_orient(x)

        lh = self.strip_h_large(x_oriented)
        lw = self.strip_w_large(x_oriented)
        sh = self.strip_h_small(x_oriented)
        sw = self.strip_w_small(x_oriented)
        loc3 = self.local_3x3(x)
        loc5 = self.local_5x5(x)

        branches = [lh, lw, sh, sw, loc3, loc5]

        # ==========================================
        # V2ï¼šç©ºé—´é—¨æ§ï¼Œ[B, 6, H, W] åƒç´ çº§åˆ†æ”¯ç«äº‰
        # ç»†é•¿åƒç´ æ¿€æ´»é•¿æ¡åˆ†æ”¯ï¼Œæ‹è§’åƒç´ æ¿€æ´»å±€éƒ¨åˆ†æ”¯
        # ==========================================
        cat_feat = torch.cat(branches, dim=1)
        spatial_weight = self.spatial_gating(cat_feat)          # [B, 6, H, W]
        spatial_weight = F.softmax(spatial_weight, dim=1)       # åœ¨6ä¸ªåˆ†æ”¯ç»´åº¦ä¸Šç«äº‰

        stacked = torch.stack(branches, dim=1)                  # [B, 6, C, H, W]
        spatial_weight = spatial_weight.unsqueeze(2)            # [B, 6, 1, H, W]
        out = (spatial_weight * stacked).sum(dim=1)             # [B, C, H, W]

        # ==========================================
        # æ¢å¤ V1ï¼Œé™„åŠ æ•°å€¼å®‰å…¨è¡¥ä¸ï¼šåŠ¨æ€é€šé“ç¼©æ”¾
        # ==========================================
        b, c, h, w = out.shape
        out_flat = out.view(b, c, -1)
        
        # æå–ç‰¹å¾
        feat_mean = out_flat.mean(dim=2, keepdim=True).unsqueeze(-1) # [B, C, 1, 1]
        var = out_flat.var(dim=2, keepdim=True, unbiased=False)
        # ğŸš¨ æå…¶å…³é”®çš„é˜²å´©è¡¥ä¸ï¼š+ 1e-5
        feat_std = torch.sqrt(var + 1e-5).unsqueeze(-1) # [B, C, 1, 1]
        
        style_feat = torch.cat([feat_mean, feat_std], dim=1)
        
        gamma_beta = self.style_fc(style_feat)
        gamma, beta = torch.chunk(gamma_beta, 2, dim=1)
        gamma = 2.0 * torch.sigmoid(gamma) 
        
        # ä¸åš InstanceNormï¼ä¿ç•™ä½ çš„ç»å¯¹å¼ºåº¦ä¿¡æ¯ï¼
        out = gamma * out + beta
        
        out = self.proj_out(out)

        return shortcut + out