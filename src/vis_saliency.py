import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np
import cv2
import os
import sys
import types # <--- å¼•å…¥è¿™ä¸ªåº“æ¥å®žçŽ°åŠ¨æ€æ›¿æ¢

sys.path.append(".") 
from model import ST_SAM

# =================é…ç½®åŒºåŸŸ=================
IMG_PATH = r"/workspace/data/root/xiaoju/Unet/dataset/Infrared3/Original/Infrared3_000012.PNG" 
CHECKPOINT = "./checkpoints/sam2_hiera_large.pt"
ST_SAM_CKPT = "./checkpoints/fold_0/best_model.pth" 
device = "cuda" if torch.cuda.is_available() else "cpu"
# =========================================

# 1. å®šä¹‰ä¸€ä¸ªæ–°çš„ forward å‡½æ•°ï¼ŒåŽ»æŽ‰äº† torch.no_grad()
def forward_with_grad(self, images, box_prompts):
    """
    è¿™æ˜¯ä¸€ä¸ªâ€œç ´è§£ç‰ˆâ€çš„å‰å‘ä¼ æ’­ï¼Œä¸“é—¨ç”¨äºŽ Saliency Map è®¡ç®—ã€‚
    å®ƒåŽ»æŽ‰äº† Image Encoder çš„ no_grad é™åˆ¶ï¼Œå…è®¸æ¢¯åº¦å›žä¼ åˆ°è¾“å…¥å›¾åƒã€‚
    """
    # 1. Image Encoder (æ³¨æ„ï¼šè¿™é‡ŒåŽ»æŽ‰äº† with torch.no_grad():)
    backbone_out = self.sam2.image_encoder(images)
    src_features = backbone_out["vision_features"]
    
    _fpn_features = backbone_out["backbone_fpn"]
    raw_s0 = _fpn_features[0]
    raw_s1 = _fpn_features[1]

    # 2. High-Res Injection (Trainable)
    feat_s0 = self.proj_s0(raw_s0)
    feat_s1 = self.proj_s1(raw_s1)
    
    refined_s0 = self.adapter_s0(feat_s0) 
    refined_s1 = self.adapter_s1(feat_s1)
    
    high_res_features = [refined_s0, refined_s1]

    # 3. Prompt Encoder
    sparse_embeddings, dense_embeddings = self.sam2.sam_prompt_encoder(
        points=None,
        boxes=box_prompts,
        masks=None,
    )

    # 4. Mask Decoder
    low_res_masks, iou_predictions, _, _ = self.sam2.sam_mask_decoder(
        image_embeddings=src_features,
        image_pe=self.sam2.sam_prompt_encoder.get_dense_pe(),
        sparse_prompt_embeddings=sparse_embeddings,
        dense_prompt_embeddings=dense_embeddings,
        multimask_output=False,
        repeat_image=False,
        high_res_features=high_res_features 
    )
    
    # 5. Upscale
    masks = F.interpolate(
        low_res_masks, 
        size=(images.shape[2], images.shape[3]), 
        mode="bilinear", 
        align_corners=False
    )
    
    return masks

def get_saliency_map(model, img_tensor, box):
    # 1. å¼€å¯è¾“å…¥å›¾åƒçš„æ¢¯åº¦è®°å½•
    img_tensor.requires_grad = True
    
    # 2. å‰å‘ä¼ æ’­
    preds = model(img_tensor, box)
    
    # 3. é€‰å–æˆ‘ä»¬è¦è§£é‡Šçš„ç›®æ ‡ (æœ€å¤§åŒ– Mask å“åº”)
    score = torch.sigmoid(preds)
    target = score.sum()
    
    # 4. åå‘ä¼ æ’­
    model.zero_grad()
    target.backward()
    
    # 5. èŽ·å–æ¢¯åº¦
    if img_tensor.grad is None:
        return None
        
    gradients = img_tensor.grad.data.abs().squeeze(0).cpu().numpy()
    
    # 6. å¤„ç†æ¢¯åº¦ (RGB æœ€å¤§å€¼)
    saliency = np.max(gradients, axis=0)
    
    # å½’ä¸€åŒ–
    saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min() + 1e-8)
    return saliency

def visualize_saliency():
    print("â³ Loading model...")
    model = ST_SAM(checkpoint_path=CHECKPOINT).to(device)
    
    # åŠ è½½æƒé‡
    state_dict = torch.load(ST_SAM_CKPT, map_location=device)
    new_state = {k.replace("module.", ""): v for k, v in state_dict.items()}
    model.load_state_dict(new_state)
    
    # ã€å…³é”®é­”æ³•ã€‘åŠ¨æ€æ›¿æ¢ forward æ–¹æ³•
    # è¿™è¡Œä»£ç æŠŠæ¨¡åž‹å®žä¾‹çš„ forward æ–¹æ³•æ¢æˆäº†æˆ‘ä»¬çš„ forward_with_grad
    model.forward = types.MethodType(forward_with_grad, model)
    print("ðŸ”“ Model forward method patched (Gradient flow unlocked).")

    model.eval()
    
    # è¯»å–å›¾ç‰‡
    img_bgr = cv2.imread(IMG_PATH)
    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
    
    input_img = cv2.resize(img_rgb, (1024, 1024))
    img_tensor = torch.from_numpy(input_img).permute(2, 0, 1).float() / 255.0
    img_tensor = img_tensor.unsqueeze(0).to(device)
    box = torch.tensor([[0, 0, 1024, 1024]], device=device).float()

    print("ðŸš€ Calculating Saliency...")
    # ç®€å•çš„ SmoothGrad æ¨¡æ‹Ÿï¼šç¨å¾®åŠ ä¸€ç‚¹å™ªå£°æ±‚å¹³å‡ï¼Œå›¾ä¼šæ›´å¹²å‡€
    saliency_total = np.zeros((1024, 1024))
    n_samples = 5 # è·‘5æ¬¡æ±‚å¹³å‡
    
    for i in range(n_samples):
        # åŠ å¾®å°å™ªå£°
        noise = torch.randn_like(img_tensor) * 0.02
        curr_img = img_tensor + noise
        curr_map = get_saliency_map(model, curr_img, box)
        if curr_map is not None:
            saliency_total += curr_map
            
    saliency = saliency_total / n_samples
    
    # === ç»˜å›¾ ===
    plt.figure(figsize=(12, 6))
    
    # 1. åŽŸå›¾
    plt.subplot(1, 2, 1)
    plt.imshow(img_rgb)
    plt.title("Original Infrared Input\n(Note: Strong Rings)", fontsize=14)
    plt.axis("off")
    
    # 2. Saliency Map
    plt.subplot(1, 2, 2)
    # å¢žå¼ºå¯¹æ¯”åº¦ï¼šGamma Correction
    saliency_vis = np.power(saliency, 0.6) 
    
    plt.imshow(img_rgb, alpha=0.6) # åº•å›¾å˜æ·¡
    plt.imshow(saliency_vis, cmap='jet', alpha=0.7) # çƒ­åŠ›å›¾
    plt.title("Gradient Saliency Map\n(Red = High Importance)", fontsize=14, color='darkred', fontweight='bold')
    plt.axis("off")
    
    save_path = "vis_saliency_v2.png"
    plt.savefig(save_path, bbox_inches='tight', dpi=300)
    print(f"âœ… Visualization saved to {save_path}")
    plt.show()

if __name__ == "__main__":
    visualize_saliency()